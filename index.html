
    <html>
    <head>
    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"
          integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <link href='http://fonts.googleapis.com/css?family=Lato:300,400,900' rel='stylesheet' type='text/css'>
    <link href="style.css" rel="stylesheet">
    <title>Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models</title>
    </head>
    <body>

        <div id="header" class="container-fluid">
            <div class="row" style="text-align: center;"/>
            <!--<div class="logoimg">
                <img src="meta_logo.png" height="95">
                <img src="huji_logo.png" height="95">
            </div>-->
                <h1>Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models</h1>
                                <div class="authors">
                   <a href="https://rongjiehuang.github.io">Rongjie Huang*</a><sup>1</sup>,
                    <a href="">Jiawei Huang*</a><sup>1</sup>,
                    <a href="http://dongchaoyang.top">Dongchao Yang*</a><sup>2</sup>,
                    <a href="https://rayeren.github.io/">Yi Ren</a><sup>3</sup>,
                   <a href="https://luping-liu.github.io/">Luping liu</a><sup>1</sup>,
                    <a href="">Mingze Li</a><sup>1</sup>,
                    <a href="https://yerfor.github.io/en/">Zhenhui Ye</a><sup>1</sup>,
                    <a href="https://silentlin15.github.io/">Jinglin Liu</a><sup>1</sup>,
                    <a href="">Xiang Yin</a><sup>3</sup>,
                    <a href="https://scholar.google.com/citations?user=IIoFY90AAAAJ">Zhou Zhao</a><sup>1</sup>,
                </div>

                <br>
                <p><sup>1</sup>Zhejiang University <sup>2</sup>Peking University <sup>3</sup>Speech & Audio Team, ByteDance AI Lab</p>
                <p><sup>*</sup>Equal Contribution</p>

                 <div class="assets">
                    <a href="paper.pdf" target="_blank">[paper]</a><a href="https://github.com/Text-to-Audio/Make-An-Audio" target="_blank">[github]</a>
                </div>
                
        </div>

    <div class="container">
        <h2>Abstract</h2>
        <span>
            Large-scale multimodal generative modeling has created milestones in text-to-image and text-to-video generation. Its application to audio still lags behind due to two main reasons: the lack of large-scale datasets with high-quality text-audio pairs, and the complexity of modeling long continuous audio data. In this work, we propose Make-An-Audio with a prompt-enhanced diffusion model that addresses these gaps by 1) introducing pseudo prompt enhancement with a distill-then-reprogram approach which alleviates the data scarcity by using weekly-supervised data with language-free audios; 2) leveraging spectrogram autoencoder to predict the self-supervised audio representation instead of waveforms. Together with robust contrastive language-audio pretraining (CLAP) representations, Make-An-Audio achieves state-of-the-art results in both objective and subjective evaluation. Moreover, we present its controllability with classifier-free guidance and generalization for X-to-Audio with "No Modality Left Behind", for the first time unlocking the ability to generate high-definition, high-fidelity audios given a user-defined modality input.
        </span>
    </div>

    
      

    </body>
    </html>
    
